{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eac47a46",
   "metadata": {},
   "source": [
    "## 1- **Importing Data from KaggleHub**\n",
    "\n",
    "After downloading the dataset with KaggleHub, the files are stored locally inside the `path` folder.  \n",
    "Often, Kaggle datasets contain one or more `.csv` files, so we need to load them into Pandas dataframes, and then combine them into one large dataframe.\n",
    "\n",
    "```python\n",
    "import os\n",
    "import glob\n",
    "csv_files = glob.glob(path + \"/*.csv\")         # Get a list of all CSV files in the dataset folder\n",
    "dfs = [pd.read_csv(f) for f in csv_files]      # Read each CSV file into a Pandas DataFrame\n",
    "df_combined = pd.concat(dfs, ignore_index=True) # Combine them into one large DataFrame, resetting the index\n",
    "print(df_combined.shape)                       # Show rows x columns (size of dataset)\n",
    "print(df_combined.head())                      # Show the first 5 rows of the dataset\n",
    "```\n",
    "\n",
    "If I have more than one file from different resources, I can add the name of each resource as a new column to the dataframe, so I can keep track of where each row came from.\n",
    "\n",
    "```python\n",
    "dfs = [pd.read_csv(file).assign(brand=os.path.splitext(os.path.basename(file))[0]) for file in csv_files]\n",
    "df_combined = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c90022",
   "metadata": {},
   "source": [
    "## 2- **Data Cleaning and Preprocessing**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2456f48",
   "metadata": {},
   "source": [
    "Before we start modifying the data (dropping columns, transforming, etc.), we need to understand the dataset first, and that's why we use the EDA (Exploratory Data Analysis) process.\n",
    "\n",
    "- **What is EDA?** It's a process of analyzing the dataset to summarize its main characteristics, often using visual methods. The goal of EDA is to understand the data better, identify patterns, spot anomalies, test hypotheses, and check assumptions.\n",
    "- **Why is EDA important?** Because it helps us to make informed decisions about how to clean and preprocess the data, which features to use for modeling, and which algorithms to apply. It also helps us to identify potential problems with the data, such as missing values, outliers, and multicollinearity.\n",
    "- **How to perform EDA?** There are many techniques and tools available for EDA, but some common ones include:\n",
    "  - Descriptive statistics: mean, median, mode, standard deviation, etc.\n",
    "  - Data visualization: histograms, box plots, scatter plots, heatmaps, etc.\n",
    "  - Correlation analysis: Pearson correlation, Spearman correlation, etc.\n",
    "\n",
    "For more information, please refer to the [EDA](https://www.geeksforgeeks.org/data-analysis/what-is-exploratory-data-analysis/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d9e5e4",
   "metadata": {},
   "source": [
    "![mlconcepts_image7](/S1-Intro_to_ML/images/EDA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a61168",
   "metadata": {},
   "source": [
    "### 1- **Dropping Unnecessary Columns**\n",
    "After performing EDA, we can drop any columns that are not needed for our analysis or modeling. This can help to reduce the size of the dataset and improve the performance of our models.\n",
    "\n",
    "```python\n",
    "df.drop(columns=['unnecessary_column_1', 'unnecessary_column_2'], axis=1) # Drop unnecessary columns, axis=1 means columns\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2085314",
   "metadata": {},
   "source": [
    "### 2- **Handling Missing Values**\n",
    "if we have missing values in our dataset (NA, NaN, None, etc.), the models may not work properly, so we need to handle them before training our models.\n",
    "Missing values can be handled in several ways, depending on the nature of the data and the extent of the missingness.<br>\n",
    "Some common techniques include: Removing rows or columns with missing values, or imputing missing values with mean, median, mode, or using more advanced techniques like KNN imputation or regression imputation\n",
    "\n",
    "- **What is imputation?** Imputation is the process of replacing missing values in a dataset with estimated values.\n",
    "- **Why is imputation important?** Because many machine learning algorithms cannot handle missing values, and removing rows or columns with missing values can lead to loss of valuable information and reduced dataset size.\n",
    "- **How to perform imputation?** There are several techniques for imputation, including:\n",
    "  - Simple imputation: Replacing missing values with the mean, median, mode, or a constant value.\n",
    "  - KNN imputation: Using the k-nearest neighbors algorithm to estimate missing values based on the values of similar rows.\n",
    "  - Regression imputation: Using regression models to predict missing values based on other features in the dataset.\n",
    "\n",
    "\n",
    "##### **Handling Missing Values with SimpleImputer**\n",
    "- **Numeric Data**: \n",
    "  - **Mean** → average of all values <br>\n",
    "  Formula: $\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n} x_i$ <br>\n",
    "  Useful when data is normally distributed (no big outliers). \n",
    "\n",
    "  - **Median** → middle value when data is sorted <br>\n",
    "  Example: `[1, 3, 4, 9, 12] → median = 4`<br>\n",
    "  Better than mean when data has **outliers** (skewed distribution).\n",
    "\n",
    "- **Categorical Data**: \n",
    "  - **Mode** → most frequent value <br>\n",
    "  Example: `[red, blue, blue, green] → mode = blue`<br>\n",
    "  Works well if missing values are many, since the most frequent label is likely the best guess.\n",
    "\n",
    "  - **Constant value** → replace with a fixed label (e.g., `\"Unknown\"`, `\"N/A\"`) <br>\n",
    "  This is useful if missing values are few → instead of forcing them into an existing category, we keep them separate.\n",
    "\n",
    "```python\n",
    "from sklearn.impute import SimpleImputer\n",
    "print(df_combined.isnull().sum())\n",
    "# For numeric columns\n",
    "num_imputer = SimpleImputer(strategy='median') \n",
    "df[numeric_cols] = num_imputer.fit_transform(df[numeric_cols])\n",
    "# For categorical columns\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "df[categorical_cols] = cat_imputer.fit_transform(df[categorical_cols])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8489ed",
   "metadata": {},
   "source": [
    "### 3- **Encoding Categorical Variables**\n",
    "Many machine learning algorithms require numerical input, so we need to convert categorical variables into numerical format. There are several techniques for encoding categorical variables, including:\n",
    "- **One-Hot Encoding**: This technique creates a new binary column for each category in the original column. For example, if we have a column \"Color\" with categories \"Red\", \"Blue\", and \"Green\", one-hot encoding will create three new columns: \"Color_Red\", \"Color_Blue\", and \"Color_Green\". Each row will have a value of 1 in the column corresponding to its category and 0 in the other columns.\n",
    "- **Label Encoding**: This technique assigns a unique integer to each category in the original column. For example, if we have a column \"Color\" with categories \"Red\", \"Blue\", and \"Green\", label encoding will assign the values 0, 1, and 2 to these categories, respectively. This technique is useful when there is an ordinal relationship between the categories (e.g., \"Low\", \"Medium\", \"High\").\n",
    "- **Target Encoding**: This technique replaces each category in the original column with the mean of the target variable for that category. For example, if we have a column \"Color\" and a target variable \"Price\", target encoding will replace each category in the \"Color\" column with the mean price for that color. This technique can be useful when there is a strong relationship between the categorical variable and the target variable.\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "categorical_cols=[\"model\",\"transmission\",\"fuelType\"]\n",
    "encoder=OneHotEncoder(drop='first', sparse_output=False) # drop='first' to avoid dummy variable trap, sparse_output=False to get a dense array\n",
    "encoded_data=encoder.fit_transform(df_combined[categorical_cols])\n",
    "encoded_df=pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(categorical_cols))\n",
    "df_combined=pd.concat([df_combined, encoded_df], axis=1)\n",
    "df_combined=df_combined.drop(categorical_cols, axis=1) # Drop original categorical columns to avoid errors\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbd2608",
   "metadata": {},
   "source": [
    "![mlconcepts_image8](/S1-Intro_to_ML/images/OHE.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff46160b",
   "metadata": {},
   "source": [
    "### 4- **Removing Outliers**\n",
    "Outliers are data points that are significantly different from the rest of the data. They can be caused by measurement errors, data entry errors, or natural variability in the data. Outliers can have a significant impact on the performance of machine learning models, so it is important to identify and remove them before training our models.\n",
    "- **How to identify outliers?** There are several techniques for identifying outliers, including:\n",
    "  - Visual inspection: Plotting the data using box plots, scatter plots, or histograms can help to identify outliers visually.\n",
    "  - Statistical methods: Using statistical measures such as Z-scores or the IQR (Interquartile Range) can help to identify outliers mathematically.\n",
    "\n",
    "\n",
    "Here are common **statistical methods** for handling outliers:\n",
    "#### 1. Z-Score Method (Standard Deviation Method)\n",
    "\n",
    "**Formula:** \n",
    "$$ Z = \\frac{x - \\mu}{\\sigma} $$\n",
    "Where:\n",
    "- $ {x} $ : data point  \n",
    "- $ {\\mu} $ : mean of the dataset  \n",
    "- $ {\\sigma} $ : standard deviation  \n",
    "\n",
    "**Rule of thumb:** if \\(|Z| > 3\\), the point is an outlier.  \n",
    "**Best for:** **Normally distributed numerical data**.  \n",
    "\n",
    "#### 2. IQR Method (Interquartile Range)\n",
    "**Formulas:**\n",
    "$$ IQR = Q_3 - Q_1 $$\n",
    "$$ \\text{Lower Bound} = Q_1 - 1.5 \\times IQR $$\n",
    "$$ \\text{Upper Bound} = Q_3 + 1.5 \\times IQR $$\n",
    "\n",
    "- $ {Q1} $: 25th percentile  \n",
    "- $ {Q3} $: 75th percentile  \n",
    "\n",
    "Any point outside $[Lower, Upper]$ is considered an outlier.   \n",
    "**Best for:** **Skewed numerical data**, robust to non-normal distributions.  \n",
    "\n",
    "#### 3. Modified Z-Score (using Median and MAD)\n",
    "**Formula:**\n",
    "$$ M_i = \\frac{0.6745 \\, (x_i - \\text{Median})}{MAD} $$\n",
    "Where: $$ MAD = \\text{Median} \\big( |x_i - \\text{Median}| \\big) $$ \n",
    "\n",
    "- Threshold: if $|M_i| > 3.5$, the point is an outlier.  \n",
    "**Best for:** **Numerical data with heavy skewness or outliers** (more robust than mean/std).  \n",
    "\n",
    "#### 4. Domain / Business Rules\n",
    "Sometimes, outliers are defined by **real-world knowledge**.  \n",
    "Example: age cannot be negative, or engine size cannot be > 10L in a car dataset.  \n",
    "\n",
    "\n",
    "```python\n",
    "def remove_outliers(df,col):\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    df_filtered = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]\n",
    "    return df_filtered\n",
    "\n",
    "numerical_cols = df_combined.select_dtypes(include=['float64', 'int64']).columns\n",
    "for col in numerical_cols:\n",
    "    df_combined = remove_outliers(df_combined, col)\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d02bd4",
   "metadata": {},
   "source": [
    "![mlconcepts_image9](/S1-Intro_to_ML/images/IQR.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0b0122",
   "metadata": {},
   "source": [
    "### 5- **Feature Engineering**\n",
    "Feature engineering is the process of creating, modifying, or transforming variables (features) in a dataset to help machine learning models capture patterns more effectively.\n",
    "\n",
    "**How do we do Feature Engineering?**  \n",
    "- **Domain Knowledge:** Use common sense or subject knowledge to create meaningful features.  \n",
    "- **Combining Features:** Derive new ones from existing ones.  \n",
    "- **Encoding Categorical Data:** Convert categories into numbers.  \n",
    "- **Transformations:** Apply mathematical changes (log, square root) to reduce skewness.  \n",
    "- **Extracting Information:** Break down dates, text, or images into useful variables.  \n",
    "- **Evaluation:** Use correlation, feature importance, or model performance to check if the new features are useful.\n",
    "\n",
    "**Why is Feature Engineering Important?**  \n",
    "- **Improve accuracy**: Choosing the right features helps the model learn better, leading to more accurate predictions  \n",
    "- **Reduce overfitting**: Using fewer, more important features helps the model avoid memorizing the data and perform better on new data.  \n",
    "- **Boost interpretability**: Well-chosen features make it easier to understand how the model makes its predictions.  \n",
    "- **Enhance efficiency**: Focusing on key features speeds up the model’s training and prediction process, saving time and resources.\n",
    "\n",
    "**What is Correlation?**\n",
    "Correlation measures the **strength and direction** of a linear relationship between two variables.\n",
    "The **Pearson correlation coefficient** is defined as:\n",
    "$$\n",
    "\\rho_{X,Y} = \\frac{\\text{cov}(X, Y)}{\\sigma_X \\, \\sigma_Y}\n",
    "$$\n",
    "\n",
    "where  \n",
    "- $\\text{cov}(X, Y)$ is the covariance between variables $X$ and $Y$ \n",
    "- $\\sigma_X$ and $\\sigma_Y$ are the standard deviations of $X$ and $Y$, respectively.  \n",
    "- The correlation coefficient ranges $\\rho_{X,Y}$ from -1 to +1:\n",
    "  - +1 indicates a perfect positive linear relationship (as one variable increases, the other also increases).\n",
    "  - -1 indicates a perfect negative linear relationship (as one variable increases, the other decreases).\n",
    "  - 0 indicates no linear relationship between the variables.\n",
    "\n",
    "When two features are **highly correlated** (positively or negatively), they provide almost the **same information** to the model.  \n",
    "Keeping both does not add value — it can lead to **multicollinearity**, which can confuse the model and cause overfitting.\n",
    "\n",
    "```python\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(12, 8))\n",
    "correlation_matrix = df_combined.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap='coolwarm')\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "For more information, please refer to the [Feature Engineering](https://www.geeksforgeeks.org/machine-learning/what-is-feature-engineering/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76549b4a",
   "metadata": {},
   "source": [
    "![mlconcepts_image10](/S1-Intro_to_ML/images/FE.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b923bf8",
   "metadata": {},
   "source": [
    "### 6-**Splitting the Dataset**-**Train-Test Split**\n",
    "Before training our machine learning models, we need to split our dataset into training and testing sets. This allows us to evaluate the performance of our models on unseen data.\n",
    "\n",
    "```python\n",
    "x=df_combined.drop('price', axis=1)\n",
    "y=df_combined['price']\n",
    "print(y.mean())\n",
    "print(y.median()) # just to recheck\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(x, y, test_size=0.4, random_state=42)  # 60% train\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)  # 20% val, 20% test\n",
    "#As for the random_state, you can use any integer value. It is just a seed for the random number generator to ensure reproducibility.\n",
    "````\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31934363",
   "metadata": {},
   "source": [
    "### 7- **Feature Scaling**\n",
    "Feature scaling is a technique used to standardize the range of independent variables or features of data. In machine learning, it is important because many algorithms are sensitive to the scale of the input features. If the features are on different scales, the algorithm may give more weight to the features with larger values, leading to biased results.\n",
    "\n",
    "There are several methods for feature scaling:\n",
    "\n",
    "1. **Min-Max Scaling**: This technique scales the data to a fixed range, usually 0 to 1. The formula is:\n",
    "   $$\n",
    "   X' = \\frac{X - X_{min}}{X_{max} - X_{min}}\n",
    "   $$\n",
    "\n",
    "2. **Standardization (Z-score Normalization)**: This method scales the data to have a mean of 0 and a standard deviation of 1. The formula is:\n",
    "   $$\n",
    "   X' = \\frac{X - \\mu}{\\sigma}\n",
    "   $$\n",
    "   where $\\mu$ is the mean and $\\sigma$ is the standard deviation.\n",
    "\n",
    "3. **Robust Scaling**: This technique uses the median and the interquartile range for scaling, making it robust to outliers. The formula is:\n",
    "   $$\n",
    "   X' = \\frac{X - X_{median}}{X_{IQR}}\n",
    "   $$\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "````"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
